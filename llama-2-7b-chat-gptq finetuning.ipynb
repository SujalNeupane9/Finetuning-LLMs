{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate peft trl datasets bitsandbytes auto-gptq optimum -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T02:02:37.501316Z","iopub.execute_input":"2023-12-06T02:02:37.501599Z","iopub.status.idle":"2023-12-06T02:03:00.299695Z","shell.execute_reply.started":"2023-12-06T02:02:37.501572Z","shell.execute_reply":"2023-12-06T02:03:00.298565Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = torch.cuda.get_device_name()\n    print(f\"CUDA device: {device}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"CUDA is not available on this system.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:03:00.301693Z","iopub.execute_input":"2023-12-06T02:03:00.302028Z","iopub.status.idle":"2023-12-06T02:03:05.708699Z","shell.execute_reply.started":"2023-12-06T02:03:00.301993Z","shell.execute_reply":"2023-12-06T02:03:05.707725Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CUDA device: Tesla T4\nCUDA version: 11.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport datasets\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,GPTQConfig, TrainingArguments\nfrom peft import LoraConfig,prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:03:05.709854Z","iopub.execute_input":"2023-12-06T02:03:05.710312Z","iopub.status.idle":"2023-12-06T02:03:27.026700Z","shell.execute_reply.started":"2023-12-06T02:03:05.710266Z","shell.execute_reply":"2023-12-06T02:03:27.025657Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/chatml_synthia\",split='train')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:03:27.029592Z","iopub.execute_input":"2023-12-06T02:03:27.029974Z","iopub.status.idle":"2023-12-06T02:03:34.984531Z","shell.execute_reply.started":"2023-12-06T02:03:27.029938Z","shell.execute_reply":"2023-12-06T02:03:34.983476Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset parquet/TokenBender--chatml_synthia to /root/.cache/huggingface/datasets/parquet/TokenBender--chatml_synthia-7986091d90ed1edd/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"099776322e50468dbacad423718b8dbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/252M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43425bf439a74037a39f8c6751ee0d47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c726ea5f7b354ad8b09cf2e9b4cf5c35"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/TokenBender--chatml_synthia-7986091d90ed1edd/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['text'][0]","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:03:34.985902Z","iopub.execute_input":"2023-12-06T02:03:34.986269Z","iopub.status.idle":"2023-12-06T02:03:35.548592Z","shell.execute_reply.started":"2023-12-06T02:03:34.986235Z","shell.execute_reply":"2023-12-06T02:03:35.547465Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nSolve this riddle: \"I am seen both in art and nature, appearing in shells and flowers alike. I grow by a specific sequence, where each number is the sum of the two before. What am I?\"<|im_end|>\\n<|im_start|>assistant\\nBreaking down the riddle:\\n\\n1. \"I am seen both in art and nature, appearing in shells and flowers alike.\" This part suggests that the answer is a pattern or a form that can be observed in both natural and man-made objects.\\n\\nIntermediate thoughts:\\n- Thought 1: This could refer to a symmetrical pattern, as symmetry is often seen in both art and nature.\\n- Thought 2: This could also refer to a repeating pattern, as repeating patterns are also common in art and nature.\\n- Thought 3: This could refer to a spiral pattern, as spirals are seen in both art and nature, especially in shells and flowers.\\n\\nEvaluating thoughts:\\n- Symmetry is a broad concept and does not directly connect with the other parts of the riddle, making it less likely to be the answer.\\n- Repeating patterns also do not directly connect with the other parts of the riddle.\\n- Spiral patterns are more specific and directly connect with the next part of the riddle, making them a likely candidate.\\n\\n2. \"I grow by a specific sequence, where each number is the sum of the two before.\" This part suggests that the answer is a mathematical sequence where each term is the sum of the two preceding ones.\\n\\nIntermediate thoughts:\\n- Thought 1: This could refer to the arithmetic sequence, but in an arithmetic sequence, each term is the sum of the previous term and a constant difference, not the sum of the two preceding terms.\\n- Thought 2: This could refer to the geometric sequence, but in a geometric sequence, each term is the product of the previous term and a constant ratio, not the sum of the two preceding terms.\\n- Thought 3: This could refer to the Fibonacci sequence, where each term is indeed the sum of the two preceding ones.\\n\\nEvaluating thoughts:\\n- Both the arithmetic sequence and the geometric sequence do not fit the description given in the riddle.\\n- The Fibonacci sequence fits the description perfectly.\\n\\nBy combining the strongest thoughts from each part of the riddle, the answer becomes clear.\\n\\nFinal Answer: The answer to the riddle is the Fibonacci sequence. This sequence is seen in both art and nature, particularly in the spiral patterns of shells and flowers. Each number in the Fibonacci sequence is the sum of the two preceding ones, which is why it \"grows by a specific sequence.\"<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"model_ckpt = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_ckpt\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:03:35.550257Z","iopub.execute_input":"2023-12-06T02:03:35.550619Z","iopub.status.idle":"2023-12-06T02:03:36.906756Z","shell.execute_reply.started":"2023-12-06T02:03:35.550589Z","shell.execute_reply":"2023-12-06T02:03:36.905913Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d1c3c9d143f452c8f9779f84f043009"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0ee23da6ae407f9db338bf32b8c984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c274771f9c4ffbb419e8d3912191c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8e639d1a794168b9d4c217f1939380"}},"metadata":{}}]},{"cell_type":"code","source":"quantization_config = GPTQConfig(bits=4,disable_exllama=True,tokenizer=tokenizer)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_ckpt,\n    revision='main',\n    quantization_config=quantization_config,\n    device_map='auto')\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:03:36.907919Z","iopub.execute_input":"2023-12-06T02:03:36.908297Z","iopub.status.idle":"2023-12-06T02:04:12.059638Z","shell.execute_reply.started":"2023-12-06T02:03:36.908260Z","shell.execute_reply":"2023-12-06T02:04:12.058540Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7acfd59da39441d6b04367cf3596ee81"}},"metadata":{}},{"name":"stderr","text":"You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13ec169b0dd4528929c6830e9b22858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725ac68d7ffa48b0b36d7468e9ab7248"}},"metadata":{}}]},{"cell_type":"code","source":"lora_config = LoraConfig(r=16,\n                        lora_alpha=32,\n                        lora_dropout=0.05,\n                        bias='none',\n                        task_type='CAUSAL_LM',\n                        target_modules=[\n                                    \"q_proj\",\n                                    \"k_proj\",\n                                    \"v_proj\",\n                                    \"o_proj\",\n                                    \"gate_proj\",\n                                    \"up_proj\",\n                                    \"down_proj\",\n                                        ]\n)\nmodel = get_peft_model(model,lora_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:04:12.060893Z","iopub.execute_input":"2023-12-06T02:04:12.061213Z","iopub.status.idle":"2023-12-06T02:04:13.081097Z","shell.execute_reply.started":"2023-12-06T02:04:12.061186Z","shell.execute_reply":"2023-12-06T02:04:13.079992Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(output_dir='.',\n                                 dataloader_drop_last=True,\n                                 save_strategy='epoch',\n                                 num_train_epochs=1,\n                                 logging_steps=100,\n                                 max_steps=1000,\n                                 per_device_train_batch_size=1,\n                                 learning_rate=3e-4,\n                                 lr_scheduler_type='cosine',\n                                 warmup_steps=100,\n                                 fp16=True,\n                                 #gradient_accumulation_steps=2,\n                                 weight_decay=0.05,\n                                 report_to=None,\n                                 run_name='finetuning-llama2-chat-7b')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:04:13.082590Z","iopub.execute_input":"2023-12-06T02:04:13.082919Z","iopub.status.idle":"2023-12-06T02:04:16.030633Z","shell.execute_reply.started":"2023-12-06T02:04:13.082892Z","shell.execute_reply":"2023-12-06T02:04:16.029485Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(model=model,\n                    args=training_args,\n                    train_dataset = dataset,\n                    dataset_text_field='text',\n                    max_seq_length=1024,\n                    tokenizer=tokenizer,\n                    packing=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:04:16.033551Z","iopub.execute_input":"2023-12-06T02:04:16.033960Z","iopub.status.idle":"2023-12-06T02:05:55.736838Z","shell.execute_reply.started":"2023-12-06T02:04:16.033929Z","shell.execute_reply":"2023-12-06T02:05:55.735849Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/119 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361bfbbba68a4d9a84dcca8300303989"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:05:55.738252Z","iopub.execute_input":"2023-12-06T02:05:55.738653Z","iopub.status.idle":"2023-12-06T03:00:39.236500Z","shell.execute_reply.started":"2023-12-06T02:05:55.738625Z","shell.execute_reply":"2023-12-06T03:00:39.235384Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231206_020759-8ektlepa</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/neupane9sujal/huggingface/runs/8ektlepa' target=\"_blank\">finetuning-llama2-chat-7b</a></strong> to <a href='https://wandb.ai/neupane9sujal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/neupane9sujal/huggingface' target=\"_blank\">https://wandb.ai/neupane9sujal/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/neupane9sujal/huggingface/runs/8ektlepa' target=\"_blank\">https://wandb.ai/neupane9sujal/huggingface/runs/8ektlepa</a>"},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 52:01, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.468800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.278000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.215000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.179200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.230700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.177700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.136800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.215500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.104800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.055800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=1.206223030090332, metrics={'train_runtime': 3283.1446, 'train_samples_per_second': 0.305, 'train_steps_per_second': 0.305, 'total_flos': 536412217958400.0, 'train_loss': 1.206223030090332, 'epoch': 0.01})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    model_ckpt,\n    revision='main',\n    quantization_config=quantization_config,\n    device_map='auto')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T03:08:45.212168Z","iopub.execute_input":"2023-12-06T03:08:45.213091Z","iopub.status.idle":"2023-12-06T03:08:49.669954Z","shell.execute_reply.started":"2023-12-06T03:08:45.213053Z","shell.execute_reply":"2023-12-06T03:08:49.668809Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"checkpoint-1000\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T03:08:55.434855Z","iopub.execute_input":"2023-12-06T03:08:55.435730Z","iopub.status.idle":"2023-12-06T03:08:56.648673Z","shell.execute_reply.started":"2023-12-06T03:08:55.435691Z","shell.execute_reply":"2023-12-06T03:08:56.647629Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_ckpt, add_bos_token=True, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T03:09:29.042590Z","iopub.execute_input":"2023-12-06T03:09:29.043032Z","iopub.status.idle":"2023-12-06T03:09:29.273428Z","shell.execute_reply.started":"2023-12-06T03:09:29.042998Z","shell.execute_reply":"2023-12-06T03:09:29.272134Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"eval_prompt = \"Solve this riddle: 'I am seen both in art and nature, appearing in shells and flowers alike. I grow by a specific sequence, where each number is the sum of the two before. What am I?\"\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nft_model.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T03:15:21.504053Z","iopub.execute_input":"2023-12-06T03:15:21.504855Z","iopub.status.idle":"2023-12-06T03:16:37.974079Z","shell.execute_reply.started":"2023-12-06T03:15:21.504820Z","shell.execute_reply":"2023-12-06T03:16:37.973007Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Solve this riddle: 'I am seen both in art and nature, appearing in shells and flowers alike. I grow by a specific sequence, where each number is the sum of the two before. What am I?'.\nAnswer:<|im_start|>user\nThe answer to the riddle is \"Fibonacci numbers.\" Fibonacci numbers are a series of numbers that start with 0 and 1, where each subsequent number is the sum of the previous two. This pattern can be found in many natural forms, such as the spiral arrangement of seeds in sunflowers or the growth patterns of certain plants. It also appears in art, particularly in the composition of\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Output\n\nSolve this riddle: 'I am seen both in art and nature, appearing in shells and flowers alike. I grow by a specific sequence, where each number is the sum of the two before. What am I?'.\nAnswer:<|im_start|>user\nThe answer to the riddle is \"Fibonacci numbers.\" Fibonacci numbers are a series of numbers that start with 0 and 1, where each subsequent number is the sum of the previous two. This pattern can be found in many natural forms, such as the spiral arrangement of seeds in sunflowers or the growth patterns of certain plants. It also appears in art, particularly in the composition of","metadata":{}}]}